{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "premium-hydrogen",
      "metadata": {
        "id": "premium-hydrogen"
      },
      "source": [
        "# 3.2 Additional exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cubic-sustainability",
      "metadata": {
        "id": "cubic-sustainability"
      },
      "source": [
        "This notebook contains a wealth of additional exercises and projects for you to pick from, as well as some glossary. Since there are quite a few and you only have 2 hours during the tutorial, just choose to solve the ones you like the most.\n",
        "\n",
        "<!--\n",
        "- [More on cross-validation](#More-on-cross-validation)\n",
        "- [Regularisation techniques](#Regularisation-techniques)\n",
        "- [Momentum](#Momentum)\n",
        "- [Learning rate scheduling](#Learning-rate-scheduling)\n",
        "- [Batch normalisation](#Batch-normalisation)\n",
        "- [Weight initialization](#Weight-initialization)\n",
        "- [Gradient clipping](#Gradient-clipping)\n",
        "- [Warm-up steps](#Warm-Up-steps)\n",
        "- [Ensemble methods](#Ensemble-methods)\n",
        "- [Monitor and visualise](#Monitor-and-visualise)\n",
        "- [Group discussion](#Group-discussion)\n",
        "-->\n",
        "\n",
        "## More on cross-validation\n",
        "\n",
        "**Exercise 1**: As mentioned in notebook 3.1, you don't really need to write the code for cross-validation yourself. Suitable methods have already been implemented, e.g., in scikit-learn. **a)** However, to make PyTorch work with scikit-learn, you would need to wrap it in [skorch](https://skorch.readthedocs.io/en/stable/index.html). Explore skorch. **b)** Explore [RayTune](https://docs.ray.io/en/latest/tune/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ambient-organic",
      "metadata": {
        "id": "ambient-organic"
      },
      "source": [
        "## Regularization techniques\n",
        "\n",
        "You can improve your network through regularisation techniques, such as dropout or L1/L2 regularization, to prevent overfitting and enhance model generalization. You met dropout in the exercises of notebook 2.1. L1/L2 regularisation simply means that you add a penalty term to your current loss, discouraging large parameters (for a neural network, this means that you try to keep the weights small).\n",
        "\n",
        "**Exercise 2**: You can include L2 regularisation (what is that exactly?) by setting weight\\_decay to a non-zero value in your optimiser. What exactly would you need to do in your code? What does weight\\_decay represent?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "criminal-pathology",
      "metadata": {
        "id": "criminal-pathology"
      },
      "outputs": [],
      "source": [
        "# EXAMPLE\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the optimizer with L2 regularization (weight_decay)\n",
        "optimizer1 = optim.SGD(model.parameters(), lr=0.01, weight_decay=0.1)  # weight_decay is the regularization strength"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cathedral-finnish",
      "metadata": {
        "id": "cathedral-finnish"
      },
      "source": [
        "**Exercise 3**: You could also add your L2 regularisation manually. To see how you might do this, have a look at the following example from [Kaggle](https://www.kaggle.com/code/cheesleypringlesman/minimizing-loss-using-l1-regularization-in-pytorch) on L1 regularisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "under-purse",
      "metadata": {
        "id": "under-purse"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "arranged-rescue",
      "metadata": {
        "id": "arranged-rescue"
      },
      "source": [
        "## Momentum\n",
        "\n",
        "To improve convergence when using stochastic gradient descent, we can draw on the concept of momentum from physics. Thus, in momentum-based SGD, the update is influenced not only by the current gradient but also by an exponentially decaying moving average of past gradients. This way, if the optimizer has been consistently moving in a certain direction over the last few steps, it will continue to do so, building up momentum. But how much should the previous previous gradients contribute? For this purpose, you can set a hyperparameter.\n",
        "\n",
        "**Exercise 4**: How do you do this in practice in PyTorch?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "strange-quest",
      "metadata": {
        "id": "strange-quest"
      },
      "outputs": [],
      "source": [
        "# EXAMPLE\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the optimizer with momentum\n",
        "optimizer2 = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # Here, 90 per cent of the previous momentum is carried over."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interracial-franchise",
      "metadata": {
        "id": "interracial-franchise"
      },
      "source": [
        "**Exercise 5**: What does the term \"Exponential Moving Average\" cover?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "extreme-mauritius",
      "metadata": {
        "id": "extreme-mauritius"
      },
      "source": [
        "## Learning rate scheduling\n",
        "\n",
        "**Exercise 6**: [Learning rate scheduling](https://pytorch.org/docs/stable/optim.html) is a technique used during the training of neural networks where the learning rate is adjusted over time according to a predefined schedule. The goal is to improve the training process, potentially speeding up convergence, enhancing model performance, and achieving better generalization. Construct a simple coding example implementing learning rate scheduling."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "intended-index",
      "metadata": {
        "id": "intended-index"
      },
      "source": [
        "## Batch normalisation\n",
        "\n",
        "Before we pass the data to the neural network, we normalise it. However, as the input data $x$ gets transformed, passing through each layer, $x$ might very well blow up significantly. To avoid this, we can normalise the output of each layer using [nn.BatchNorm2d()](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm2d.html) after convolutional or pooling layers and [nn.BatchNorm1d()](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html) after fully connected layers (why?). This approach is called batch normalisation (see also the original article by [Ioffe and Szegedy](https://arxiv.org/abs/1502.03167)).\n",
        "\n",
        "**Exercise 7**: Explain the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gothic-lawyer",
      "metadata": {
        "id": "gothic-lawyer"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "# Define a simple CNN with Batch Normalization\n",
        "class CNNWithBatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNWithBatchNorm, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(32)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(64)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.batchnorm_fc = nn.BatchNorm1d(128)\n",
        "        self.relu_fc = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.batchnorm1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool1(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.batchnorm2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool2(x)\n",
        "\n",
        "        x = x.view(-1, 64 * 7 * 7) # Alternative to flatten\n",
        "        x = self.fc1(x)\n",
        "        x = self.batchnorm_fc(x)\n",
        "        x = self.relu_fc(x)\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "involved-rolling",
      "metadata": {
        "id": "involved-rolling"
      },
      "source": [
        "## Weight initialisation\n",
        "\n",
        "Weight initialisation is a crucial aspect of training neural networks. It involves setting the initial values of the weights in the network before training begins. Proper weight initialisation can help improve the convergence speed and the overall performance of the neural network. PyTorch does this automatically, and weight initialisation may not be explicitly included in basic examples because the default initialisation methods provided by modern deep learning frameworks are generally well-suited for many common scenarios. Frameworks like PyTorch thus use sensible default initialisation strategies, such as Xavier/Glorot initialisation for linear layers.\n",
        "\n",
        "**Exercise 8**: But you can set decide on the initialisation yourself. Check out nn.init.xavier_uniform_(). What does it do?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "continental-biology",
      "metadata": {
        "id": "continental-biology"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_features=10, out_features=5)\n",
        "        # Explicitly set Xavier/Glorot initialization for the linear layer\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleNet()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "local-cathedral",
      "metadata": {
        "id": "local-cathedral"
      },
      "source": [
        "## Gradient clipping:\n",
        "\n",
        "Gradient clipping helps to prevent exploding gradients during the optimization process. You can find the corresponding tools in [PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html). Gradient clipping is most commonly used for recurrent neural networks (RNNs) and other models that involve sequential data processing, where the vanishing or exploding gradient problem is prevalent."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "million-analyst",
      "metadata": {
        "id": "million-analyst"
      },
      "source": [
        "## Warm-up steps\n",
        "\n",
        "You can gradually increase the learning rate during the initial steps of training. This approach can help the model to converge more quickly.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "entitled-wednesday",
      "metadata": {
        "id": "entitled-wednesday"
      },
      "source": [
        "## Ensemble Methods\n",
        "\n",
        "Ensemble methods involve training multiple models and combining their predictions. The idea is that diverse models can collectively produce more accurate and robust predictions. Indeed, many machine learning models draw on ensemble methods (cf. random forest, boosting and bagging). Also, in Deep Learning, you can find various ensemble methods. These include but are not limited to\n",
        "\n",
        "- Model Averaging: Train multiple instances of the same deep learning architecture with different random initializations or hyperparameters and average the predictions of these models during inference.\n",
        "- Bagging with neural networks: Train multiple instances of the same neural network on different subsets of the training data and average predictions during inference.\n",
        "- Weight averaging: Instead of combining predictions at the decision level (as in voting or stacking), weight averaging involves combining the weights of multiple trained models to create a single model with averaged weights.\n",
        "\n",
        "**Exercise 9**: Investigate this topic further. What does PyTorch offer in this regard (discuss briefly, e.g. code found [here](https://pytorch.org/docs/stable/optim.html) and [here](https://pytorch.org/tutorials/intermediate/ensembling.html#:~:text=Model%20ensembling%20combines%20the%20predictions,vmap%20.))?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "charming-handle",
      "metadata": {
        "id": "charming-handle"
      },
      "source": [
        "# Monitor and visualise"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dangerous-success",
      "metadata": {
        "id": "dangerous-success"
      },
      "source": [
        "TensorBoard can be used with PyTorch to visualise and analyse the training of neural networks. It offers real-time visualisation of training metrics, and it includes an interactive interface. Moreover, you can easily compare multiple training runs or experiments in TensorBoard.\n",
        "\n",
        "**Exercise 10**: Explore Tensorboard for PyTorch (e.g. [here](https://pytorch.org/tutorials/recipes/recipes/tensorboard_with_pytorch.html) and [here](\n",
        "https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/tensorboard_with_pytorch.ipynb))."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "floppy-heart",
      "metadata": {
        "id": "floppy-heart"
      },
      "source": [
        "## Group discussion\n",
        "\n",
        "**Exercise 11**: Explore the homepage of PyTorch and the Kaggle. Find at least three useful examples and references that you have not been pointed to in these notebooks and discuss them in the group.\n",
        "\n",
        "**Exercise 12**: Discuss the content of today's lecture and the notebooks. What are the main concepts to take home? Are there any aspects (of the content or PyThon programming) that you feel you need to dive further into before watching the next lecture? Discuss in the group.\n",
        "\n",
        "**Exercise 13**: Can you see any applications of the concepts presented in this course to your research?\n",
        "\n",
        "**Exercise 14**: In the lecture, we briefly discuss graph neural networks (GNN), variational autoencoders (VAE), flow-based generative models, generative adversarial networks (GAN), natural language processing (NLP), transformers, large language models (LLM), and neural density estimators. Dive into any of these topics. Find useful material, examples and references and discuss them in your group. Are any of these Deep Learning Techniques useful for your research?\n",
        "\n",
        "**Exercise 15**: Are there any Deep Learning methods that we have not mentioned or covered in the course? How do they apply to your topic of research?\n",
        "\n",
        "**Exercise 16:** Are you interested in taking the optimisation of your hyper-parameters to scale, beyond simple for-loops, as for-loops can quickly become cumbersome and inefficient? Check out [https://optuna.org/](https://optuna.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "exclusive-landscape",
      "metadata": {
        "id": "exclusive-landscape"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}